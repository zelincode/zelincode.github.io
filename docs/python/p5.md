[TOC]



### xpath

xpath**通过对html页面进行树形解析对页面元素进行分析

```python
import requests
import lxml
headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/51.0.2704.63 Safari/537.36"
    }
url = "http://go.hao123.com"
res= requests.get(url,headers=headers)
html = lxml.etree.HTML(res.text)
cc = html.xpath("/html/body/div[3]/div/div[4]/div[2]/div[1]/div[1]/a[1]/span[1]/text()")
#text()取文本
price = html.xpath("/html/body/div[3]/div/div[4]/div[2]/div[1]/div[1]/a[1]/@alog-text")
#@取属性
print(cc[0])
```

![1562742892432](C:\Users\weizelin\AppData\Roaming\Typora\typora-user-images\1562742892432.png)



```python
import requests
import lxml
headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/51.0.2704.63 Safari/537.36"
    }
url = "http://go.hao123.com"
res= requests.get(url,headers=headers)
html = lxml.etree.HTML(res.text)
#先找到特价机票的位置
tejia =html.xpath("//div[@class='tejia-air-wrapper']")

#分割为十个城市的具体内容
tejias =tejia[0].xpath(".//div[@class='tejia-air']")


#对每个城市进行处理
for tj in tejias:
    # 搜索出所有的目的地
    aa = tj.xpath(".//a[@class='flight']")
    for a in aa:
        fromto = a.xpath("./@alog-text")		#起始地点和到达地点
        price = a.xpath(".//span[@class ='tickets-price']/text()")	#价格
        print(fromto,price)
```

### Beautifulsoup

找属性有两种方法
1. soup.a['href']
2. soup.a.attrs['href']
找文本直接.text

查找有两个函数，find()和find_all()
有两个参数，一个是类别，一个是属性的键值对
soup.find('a',{'id':'link2'})

默认返回的是查找到的第一个内容

BeautifulSoup也是先将html转化成树型结构，不同与xpath的是它通过标签来筛选内容而非层级。